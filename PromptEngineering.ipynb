{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 Narges Kurkani\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "# This notebook is based on the original GenAI notebook from Google (Apache 2.0 License).\n",
    "# I have added some description and changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don’t need to be a data scientist or a machine learning engineer – everyone can write a prompt. Prompt engineering is the process of designing high-quality prompts that guide LLMs to produce accurate outputs.\n",
    "LLM output configuration: \n",
    "- Output length: the number of tokens to generate in a response. \n",
    "- Sampling controls: LLMs predict probabilities for what the next token could be.Those token probabilities are then sampled to determine what the next produced token will be.\n",
    "- Temperature: Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results.\n",
    "- Top-K: Top-K sampling selects the top K most likely tokens from the model’s predicted distribution. \n",
    "- Top-P: Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the LLM’s vocabulary).\n",
    "\n",
    "NOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM might generate text that is less relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Install the SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the exercises in this notebook will use the Gemini API. So install google-genai firstly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Set up API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can grab API key from [AI Studio](https://aistudio.google.com/app/apikey). And then choose creat API key and save it in GOOGLE_API_KEY. Free API key is available in Google AI studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = 'API_Key'\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Choose A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemini API provides access to a number of models from the Gemini model family. Using following Command You can know about models of Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Explore generation parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Output Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating text with an LLM, the length of the output impacts both cost and performance. More tokens require greater computational resources, resulting in increased energy consumption, latency, and expenses. You can specify number of output length using mac_output_tokens variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Sweet Sustenance: Why Fruits are Indispensable in Modern Life\n",
      "\n",
      "In the hustle and bustle of modern life, where processed foods and quick fixes often dominate dietary choices, the importance of fruits is frequently overlooked. Yet, these naturally occurring gifts of nature are far more than just a sweet treat. They are vital contributors to our health, well-being, and overall quality of life, offering a plethora of benefits that are indispensable in navigating the challenges of a fast-paced and often unhealthy world.\n",
      "\n",
      "One of the most compelling reasons to prioritize fruits in modern diets is their unparalleled nutritional value. Fruits are brimming with essential vitamins and minerals, vital for maintaining optimal bodily functions. Vitamin C, abundant in citrus fruits and berries, strengthens the immune system and protects against oxidative stress. Vitamin A, found in mangoes and papayas, promotes healthy vision and skin. Potassium, plentiful in bananas and avocados, helps regulate blood pressure and nerve function. These are just a few examples of the nutritional powerhouses\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "short_config = types.GenerateContentConfig(max_output_tokens=200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Write a 600 word essay on the importance of Fruits in modern life.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Temprature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature adjusts the randomness in token selection. Higher values increase diversity by considering more candidate tokens, while lower values make outputs more deterministic, with 0 resulting in greedy decoding. Though it doesn't guarantee randomness, temperature helps steer the output in a desired direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magenta\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Purple\n",
      " -------------------------\n",
      "Purple\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=high_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=low_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3. Top-P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like temperature, the top-P parameter controls output diversity by setting a probability threshold for token selection. A top-P of 0 results in greedy decoding, while 1 includes all possible tokens. Top-K, which is not adjustable in Gemini 2.0 models, limits selection to the K most probable tokens, with K=1 also leading to greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barnaby wasn't your average Beagle. Sure, he loved belly rubs and chasing squirrels, but beneath his floppy ears and wagging tail resided a spirit of boundless curiosity. He yearned for more than the familiar scent of Mrs. Higgins' lavender and the predictability of his backyard. He craved adventure, a quest worthy of his magnificent nose.\n",
      "\n",
      "One Tuesday, while Mrs. Higgins was distracted by a particularly juicy episode of \"Gardening Gurus,\" Barnaby noticed it. A glint of gold, nestled beneath the ancient oak tree. Not the usual lost button or discarded bottle cap, but something…different.\n",
      "\n",
      "He nudged it with his nose. It was a tiny, tarnished key. Not a house key, not a car key, but a key that hummed with the faintest whisper of magic. Barnaby, never one to back down from a good mystery, knew he had to find the lock it opened.\n",
      "\n",
      "His investigation began immediately. He sniffed the key, tracing its faint metallic scent. It led him to the edge of the woods bordering Mrs. Higgins' garden. He'd always been forbidden from venturing into the woods, deemed too dangerous for a \"precious little Beagle.\" But the key tugged at his heart, a siren song of the unknown.\n",
      "\n",
      "Ignoring the nagging voice of Mrs. Higgins in his head, Barnaby plunged into the emerald depths. The woods were a symphony of unfamiliar scents – damp earth, decaying leaves, and the sharp tang of wild berries. He followed the key's scent, which intensified with each step, his tail wagging furiously, his nose twitching like a radar.\n",
      "\n",
      "He navigated treacherous roots, outsmarted a grumpy badger (who, surprisingly, offered him a half-eaten apple core), and even braved a babbling brook, emerging soaked but undeterred. Finally, the scent led him to a gnarled, ancient willow tree, its weeping branches draped like a verdant curtain.\n",
      "\n",
      "Hidden amongst the roots, almost invisible beneath a blanket of moss, was a tiny, wooden door. No bigger than his head, it was crafted from the same dark wood as the key, and etched with swirling, fantastical patterns.\n",
      "\n",
      "Barnaby's heart pounded. He carefully inserted the key. It clicked. The door swung inward, revealing not a hollow tree trunk, but a shimmering portal, swirling with colors he'd never seen before.\n",
      "\n",
      "He hesitated for a moment. This was it. The adventure he'd been dreaming of. He took a deep breath, barked a single, brave bark into the unknown, and stepped through.\n",
      "\n",
      "He found himself in a meadow bathed in perpetual twilight, where fireflies danced like miniature stars and flowers bloomed in every imaginable hue. A tiny, winged creature with iridescent scales fluttered towards him.\n",
      "\n",
      "\"Welcome, Barnaby,\" it chirped. \"We've been expecting you.\"\n",
      "\n",
      "Barnaby, overwhelmed but thrilled, realized his adventure had only just begun. He had no idea what awaited him in this magical realm, but one thing was certain: this curious Beagle was ready for anything. The key had unlocked not just a door, but a whole new world of possibilities, a world where even the smallest dog could be a hero. And Barnaby, with his wagging tail and his insatiable curiosity, was ready to prove it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_config = types.GenerateContentConfig(\n",
    "    temperature=0.8,  \n",
    "    top_p=0.9,        \n",
    ")\n",
    "\n",
    "story_prompt = \"You are an imaginative writer. Create a captivating short story about a curious dog embarking on an adventure.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=content_config, \n",
    "    contents=story_prompt  \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models (LLMs) can generate text that seems remarkably human-like, but guiding them to produce specific responses can be challenging. To achieve more accurate or desired outputs, you can use various prompting techniques to direct the model's behavior more effectively. Here are a few techniques that can help you get closer to the responses you want from LLMs:\n",
    "\n",
    "\n",
    "1. A zero-shot Prompt      \n",
    "2. Enum Mode  \n",
    "3. A one-shot and few shot prompt     \n",
    "4. Json Mode  \n",
    "5. Chain of Thoughts  \n",
    "6. ReAct: Reason and Act\n",
    "7. Thinking Mode  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1. Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot prompting is when an LLM generates a response to a task without any prior examples, relying solely on its pre-trained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5,\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2. Enum Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enum mode in the Gemini API helps constrain model output to a predefined set of values, preventing unnecessary text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3. One- Shot and few- Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing one example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ),\n",
    "    contents=[few_shot_prompt, customer_order])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4. JSON Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON mode in the Gemini API enforces structured output by restricting token selection to a predefined schema, ensuring responses are in pure JSON format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
      "  \"type\": \"dessert\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ),\n",
    "    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.5. Chain of Thought (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct prompting in LLMs offers quick and efficient responses but can lead to hallucinations, where the answer seems correct but is factually or logically flawed. Chain-of-Thought prompting, which involves instructing the model to show intermediate reasoning, generally improves accuracy, especially with few-shot examples. However, it doesn't eliminate hallucinations and tends to be more costly due to increased token usage. Models like Gemini are naturally \"chatty\" and provide reasoning by default, so you may need to prompt for more direct responses if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem:\n",
       "\n",
       "1.  When you were 4, your partner was 3 times your age, so they were 4 * 3 = 12 years old.\n",
       "2.  This means your partner is 12 - 4 = 8 years older than you.\n",
       "3.  Since you are now 20, your partner is 20 + 8 = 28 years old.\n",
       "\n",
       "So the answer is 28."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.6. ReAct: Reason and act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In prompt engineering, react, reason, and act refer to specific steps to guide the behavior of language models:\n",
    "\n",
    "React: This is about setting the model up to respond quickly and appropriately to the user’s input, often based on the context and instructions provided. It focuses on immediate output without extensive reasoning.\n",
    "\n",
    "Reason: This step involves prompting the model to explain its thought process, typically through Chain-of-Thought prompting, where the model breaks down the task into logical steps. This can improve accuracy and reliability by providing intermediate reasoning before the final output.\n",
    "\n",
    "Act: This focuses on instructing the model to take action based on the reasoning or directly execute the task. It emphasizes practical application of the model's understanding to produce a result, whether that's generating text, making a decision, or performing a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper and then find the list of authors. After that, I need to find the youngest author.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "# You will perform the Action; so generate up to, but not including, the Observation.\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservation\"],\n",
    "    system_instruction=model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "# Create a chat that has the model instructions and examples pre-seeded.\n",
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=react_config,\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "I have the list of authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Now I need to find their age. Since I don't have access to their ages, I should search each name along with the keyword \"age\" or \"birthdate\" to determine their birthdates and thus, their age when the paper was published (2017). Let's start with the first author.\n",
      "\n",
      "Action 2\n",
      "<search>Ashish Vaswani age</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.7. Thinking mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemini Flash 2.0 \"Thinking\" model is designed to generate its reasoning process as part of its response, enhancing its logical capabilities without requiring specialized prompting. This approach improves response quality by incorporating intermediate \"thoughts\" into the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The youngest author listed on the original Transformers NLP paper, \"Attention is All You Need,\" is **Aidan N. Gomez**.\n",
      "\n",
      "While determining the exact birthdates of all authors to definitively say who is *absolutely* the youngest is difficult without personal information, Aidan N. Gomez was a PhD student at the University of Oxford at the time of publication (2017).  PhD students are typically younger than researchers in more senior positions at companies like Google Research (where many of the other authors were affiliated).\n",
      "\n",
      "Based on publicly available information and typical career paths, it's highly likely that **Aidan N. Gomez** was the youngest author on the paper."
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Display the response as it is streamed\n",
    "    print(chunk.text, end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Code Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1. Generating Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Gemini models love to talk, so it helps to specify they stick to the code if that\n",
    "# is all that you want.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=code_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2. Code Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemini API can automatically run generated code too, and will return the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, I can do that. First, I need to generate the first 14 odd '\n",
      "         \"prime numbers. I'll start by listing prime numbers and excluding \"\n",
      "         'even numbers (except for 2, but since we want *odd* primes, we will '\n",
      "         \"skip 2). Then, I'll sum them up.\\n\"\n",
      "         '\\n'\n",
      "         \"Here's the list of prime numbers, and I'll pick out the first 14 odd \"\n",
      "         'ones: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, '\n",
      "         '59...\\n'\n",
      "         '\\n'\n",
      "         'The first 14 odd prime numbers are: 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "         '31, 37, 41, 43, 47.\\n'\n",
      "         '\\n'\n",
      "         \"Now, let's calculate their sum using a python tool.\\n\"\n",
      "         '\\n'}\n",
      "-----\n",
      "{'executable_code': {'code': 'numbers = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n",
      "                             '37, 41, 43, 47]\\n'\n",
      "                             'sum_of_numbers = sum(numbers)\\n'\n",
      "                             'print(sum_of_numbers)\\n'\n",
      "                             '\\n',\n",
      "                     'language': 'PYTHON'}}\n",
      "-----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK', 'output': '326\\n'}}\n",
      "-----\n",
      "{'text': 'The sum of the first 14 odd prime numbers is 326.\\n'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
    ")\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Generate the first 14 odd prime numbers, then calculate their sum.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents=code_exec_prompt)\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  pprint(part.to_json_dict())\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3. Explaining code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemini family of models can explain code to you too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file, `git-prompt.sh`, is a shell script designed to enhance your command-line prompt with information about the current Git repository.\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "*   **Displays Git Status:** Shows the current branch, whether there are uncommitted changes (staged, unstaged, untracked), if your branch is ahead or behind the remote, and other relevant Git status information.\n",
       "*   **Customization:** It allows you to customize the appearance of the Git information in your prompt through theming and variable settings. You can change the colors, symbols, and even what information is displayed.\n",
       "*   **Asynchronous Fetching:** It can optionally fetch remote branch information in the background, so your prompt doesn't lag while waiting for the `git fetch` command.\n",
       "*   **Virtualenv Awareness:** It can also display information about activated Python virtual environments.\n",
       "*   **Works with Bash and Zsh:** The script aims to be compatible with both Bash and Zsh shells.\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "*   **Improved Workflow:**  The visual Git status in your prompt makes it easier to keep track of the state of your repositories, reducing the need to constantly run `git status`.\n",
       "*   **Quick Glance Information:** At a glance, you can see the branch you're on, if you have changes to commit, and whether you need to pull or push.\n",
       "*   **Enhanced Productivity:** By providing immediate feedback on your Git status, it can speed up your development workflow.\n",
       "\n",
       "In essence, `git-prompt.sh` is a tool to make your command-line prompt more informative and Git-aware, leading to a more efficient and less error-prone development experience.  You would typically source this script in your `.bashrc` or `.zshrc` file to activate its features in your shell.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=explain_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
